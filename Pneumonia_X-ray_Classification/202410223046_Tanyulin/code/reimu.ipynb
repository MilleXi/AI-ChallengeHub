{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c4a3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "目录 /root/train\\NORMAL 不存在",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m normal_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TRAIN_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNORMAL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 确保 normal_dir 存在\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(normal_dir), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m目录 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnormal_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 不存在\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 获取 NORMAL 文件夹中所有图像文件的路径\u001b[39;00m\n\u001b[0;32m     61\u001b[0m normal_images \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAssertionError\u001b[0m: 目录 /root/train\\NORMAL 不存在"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "from sklearn.utils import resample\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms, models\n",
    "import csv\n",
    "\n",
    "# ─── 宏定义 ───────────────────────────────────────────────────────────────────\n",
    "# 数据集路径\n",
    "DATA_ROOT = r\"/root/\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "TEST_DIR  = os.path.join(DATA_ROOT, \"test\")\n",
    "VAL_DIR   = os.path.join(DATA_ROOT, \"val\")\n",
    "\n",
    "# 输出目录\n",
    "OUTPUT_DIR = r\"/root/result\"\n",
    "\n",
    "# 图像处理参数\n",
    "IMG_SIZE = (224, 224)  # ResNet-50 输入大小\n",
    "BATCH_SIZE = 32  # 批大小根据显存调整\n",
    "NUM_EPOCHS = 10  # 训练轮数\n",
    "LEARNING_RATE = 0.0001  # 学习率\n",
    "SEED = 114  # 随机种子\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ─── 图像增强函数 ───────────────────────────────────────────────────────────\n",
    "def augment_image(image):\n",
    "    # 随机旋转\n",
    "    angle = np.random.randint(-10, 10)\n",
    "    rows, cols = image.shape\n",
    "    M = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)\n",
    "    rotated = cv2.warpAffine(image, M, (cols, rows))\n",
    "    \n",
    "    # 随机翻转\n",
    "    if np.random.rand() > 0.5:\n",
    "        flipped = cv2.flip(rotated, 1)  # 水平翻转\n",
    "    else:\n",
    "        flipped = rotated\n",
    "    \n",
    "    # 随机平移\n",
    "    tx = np.random.randint(-5, 5)\n",
    "    ty = np.random.randint(-5, 5)\n",
    "    M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "    translated = cv2.warpAffine(flipped, M, (cols, rows))\n",
    "    \n",
    "    return translated\n",
    "\n",
    "# ─── 过采样 NORMAL 文件夹 ───────────────────────────────────────────────────\n",
    "normal_dir = os.path.join(TRAIN_DIR, \"NORMAL\")\n",
    "# 确保 normal_dir 存在\n",
    "assert os.path.exists(normal_dir), f\"目录 {normal_dir} 不存在\"\n",
    "\n",
    "# 获取 NORMAL 文件夹中所有图像文件的路径\n",
    "normal_images = []\n",
    "for file in os.listdir(normal_dir):\n",
    "    file_path = os.path.join(normal_dir, file)\n",
    "    if os.path.isfile(file_path) and file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        normal_images.append(file_path)\n",
    "normal_labels = [0] * len(normal_images)\n",
    "\n",
    "# 如果没有找到任何图像文件，抛出异常\n",
    "if not normal_images:\n",
    "    raise FileNotFoundError(f\"在目录 {normal_dir} 中没有找到任何图像文件\")\n",
    "\n",
    "pneumonia_dir = os.path.join(TRAIN_DIR, \"PNEUMONIA\")\n",
    "# 确保 pneumonia_dir 存在\n",
    "assert os.path.exists(pneumonia_dir), f\"目录 {pneumonia_dir} 不存在\"\n",
    "\n",
    "# 获取 PNEUMONIA 文件夹中所有图像文件的路径\n",
    "pneumonia_images = []\n",
    "for file in os.listdir(pneumonia_dir):\n",
    "    file_path = os.path.join(pneumonia_dir, file)\n",
    "    if os.path.isfile(file_path) and file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        pneumonia_images.append(file_path)\n",
    "pneumonia_labels = [1] * len(pneumonia_images)\n",
    "\n",
    "# 如果没有找到任何图像文件，抛出异常\n",
    "if not pneumonia_images:\n",
    "    raise FileNotFoundError(f\"在目录 {pneumonia_dir} 中没有找到任何图像文件\")\n",
    "\n",
    "# 计算需要生成的额外 NORMAL 图像数量\n",
    "num_normal_augmented = len(pneumonia_images) - len(normal_images)\n",
    "\n",
    "augmented_normal_images = []\n",
    "for _ in range(num_normal_augmented):\n",
    "    img_path = np.random.choice(normal_images)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    augmented_img = augment_image(img)\n",
    "    augmented_normal_images.append(augmented_img)\n",
    "\n",
    "augmented_normal_dir = os.path.join(OUTPUT_DIR, \"NORMAL_augmented\")\n",
    "os.makedirs(augmented_normal_dir, exist_ok=True)\n",
    "\n",
    "for i, img in enumerate(augmented_normal_images):\n",
    "    cv2.imwrite(os.path.join(augmented_normal_dir, f\"augmented_NORMAL_{i}.png\"), img)\n",
    "\n",
    "normal_images += [os.path.join(augmented_normal_dir, img) for img in os.listdir(augmented_normal_dir)]\n",
    "normal_labels = [0] * len(normal_images)\n",
    "\n",
    "# ─── 欠采样 PNEUMONIA 文件夹 ─────────────────────────────────────────────────\n",
    "sampled_pneumonia_images, sampled_pneumonia_labels = resample(\n",
    "    pneumonia_images, pneumonia_labels,\n",
    "    n_samples=len(normal_images),\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "pneumonia_images = sampled_pneumonia_images\n",
    "pneumonia_labels = sampled_pneumonia_labels\n",
    "\n",
    "# ─── 合并过采样和欠采样后的数据集 ────────────────────────────────────────────\n",
    "all_images = normal_images + pneumonia_images\n",
    "all_labels = normal_labels + pneumonia_labels\n",
    "\n",
    "merged_dataset_dir = os.path.join(OUTPUT_DIR, \"merged_dataset\")\n",
    "os.makedirs(merged_dataset_dir, exist_ok=True)\n",
    "\n",
    "normal_merged_dir = os.path.join(merged_dataset_dir, \"NORMAL\")\n",
    "pneumonia_merged_dir = os.path.join(merged_dataset_dir, \"PNEUMONIA\")\n",
    "os.makedirs(normal_merged_dir, exist_ok=True)\n",
    "os.makedirs(pneumonia_merged_dir, exist_ok=True)\n",
    "\n",
    "for img_path in normal_images:\n",
    "    shutil.copy(img_path, normal_merged_dir)\n",
    "\n",
    "for img_path in pneumonia_images:\n",
    "    shutil.copy(img_path, pneumonia_merged_dir)\n",
    "\n",
    "# ─── 数据变换管道 ──────────────────────────────────────────────────────────\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# ─── 加载数据集 ─────────────────────────────────────────────────────────────\n",
    "# 定义一个函数来清理数据集目录\n",
    "def clean_dataset_dir(dataset_dir):\n",
    "    for subdir in os.listdir(dataset_dir):\n",
    "        subdir_path = os.path.join(dataset_dir, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            # 删除非 NORMAL 或 PNEUMONIA 的子目录\n",
    "            if subdir not in [\"NORMAL\", \"PNEUMONIA\"]:\n",
    "                shutil.rmtree(subdir_path)\n",
    "            else:\n",
    "                # 删除子目录中的无效文件\n",
    "                for file in os.listdir(subdir_path):\n",
    "                    file_path = os.path.join(subdir_path, file)\n",
    "                    if not file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        os.remove(file_path)\n",
    "\n",
    "# 清理训练集、测试集和验证集目录\n",
    "clean_dataset_dir(TRAIN_DIR)\n",
    "clean_dataset_dir(TEST_DIR)\n",
    "clean_dataset_dir(VAL_DIR)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(merged_dataset_dir, transform=data_transform)\n",
    "test_dataset  = datasets.ImageFolder(TEST_DIR,  transform=data_transform)\n",
    "val_dataset   = datasets.ImageFolder(VAL_DIR,   transform=data_transform)\n",
    "\n",
    "# 计算每个样本的采样权重，用于 WeightedRandomSampler\n",
    "class_counts = [0] * len(train_dataset.classes)\n",
    "for _, label in train_dataset.samples:\n",
    "    class_counts[label] += 1\n",
    "\n",
    "weights_per_class = [1.0 / cnt for cnt in class_counts]\n",
    "sample_weights = [weights_per_class[label] for _, label in train_dataset.samples]\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# ─── 构建 ResNet-50 模型并替换分类头 ───────────────────────────────────────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 使用 ResNet-50 模型\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 1)\n",
    "model = model.to(device)\n",
    "\n",
    "# ─── 损失函数 & 优化器 ──────────────────────────────────────────────────────\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "\n",
    "# ─── 早停机制 ────────────────────────────────────────────────────────────────\n",
    "early_stopping_patience = 3  # 如果验证损失在 3 个 epoch 内没有改善，则停止训练\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# ─── 学习率调度器 ───────────────────────────────────────────────────────────\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "\n",
    "# ─── 训练与评估 ──────────────────────────────────────────────────────────────\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs).squeeze(1)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 计算训练损失和准确率\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "        preds = torch.sigmoid(logits).round()\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += imgs.size(0)\n",
    "    avg_train_loss = train_loss / train_total\n",
    "    train_accuracy = train_correct / train_total\n",
    "\n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.float().to(device)\n",
    "            logits = model(imgs).squeeze(1)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            preds = torch.sigmoid(logits).round()\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += imgs.size(0)\n",
    "    avg_val_loss = val_loss / val_total\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # 早停机制\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "            break\n",
    "\n",
    "    # 调整学习率\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "# ─── 测试集预测并生成 submission.csv ────────────────────────────────────────\n",
    "model.eval()\n",
    "predictions = []\n",
    "image_paths = []\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "current_sample_index = 0  # 用于跟踪当前批次的起始索引\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        logits = model(imgs).squeeze(1)\n",
    "        preds = torch.sigmoid(logits).round().cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "        \n",
    "        # 获取当前批次的图像路径\n",
    "        batch_size = len(labels)\n",
    "        for i in range(batch_size):\n",
    "            img_path = test_dataset.samples[current_sample_index + i][0]\n",
    "            image_paths.append(img_path)\n",
    "        \n",
    "        # 计算测试准确率\n",
    "        test_correct += (preds == labels.cpu().numpy()).sum().item()\n",
    "        test_total += batch_size\n",
    "        \n",
    "        current_sample_index += batch_size  # 更新起始索引\n",
    "\n",
    "test_accuracy = test_correct / test_total\n",
    "\n",
    "output_path = os.path.join(OUTPUT_DIR, 'submission.csv')\n",
    "with open(output_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Image Path', 'Prediction'])\n",
    "    for img_path, pred in zip(image_paths, predictions):\n",
    "        writer.writerow([img_path, int(pred)])\n",
    "\n",
    "print(f\"submission.csv 文件已生成并保存到 {output_path}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
